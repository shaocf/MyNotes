[
["index.html", "课程笔记 本书是如何生成的？", " 课程笔记 流风邵 2020-02-21 本书是如何生成的？ 本书是介绍 R 语言包 bookdown 来完成一本书的写作，bookdown 是建立在 R Markdown 基础之上的，继承了 Markdown 语法的简易性。它通过组织一系列 R Markdown 文件来输出多种格式（如 PDF，HTML，Word 等），同时也可以增加一些特征，比如，多页 HTML 输出，对 figures/tables/sections/equations 进行编号和交叉引用 ，插入 part/appendices，导入 GitBook ( https://www.gitbook.com）类型生成好看的 HTML 书页。 如何生成一本书？ 下载 Github 仓库 bookdown-demo，然后解压，将仓库名称和工程名称修改为 MyNotes； 安装 R 包 bookdown； 使用 RStudio 编辑器打开这个工程，打开 MyCourses.Rproj，打开文件 index.Rmd，修改相对应内容； 打开 R Markdown 文件 index.Rmd，敲击 RStudio 中 Build 标签页中的 Build Book 即可生成最原始的书的形式。 格式微调 模仿书籍 R for Data Science 的 github仓库 中的文件 _bookdown.yml 来修改我们对应的同名文件。 每个 Rmd 文件只能包含一个章节，由一级标题 # 来定义； # title {-}：在一级标题后面使用 {-} ，则标题不计入章节序号； # (PART) title {-}：通过 (PART) 来体现一个部分包含几个章节； 使用_common.R文件来调整显示图像的大小和位置，将其在 _bookdown.yml中配置。 文本语法 详见 bookdown 文档。 "],
["section-1.html", "1 学习问题 1.1 什么是机器学习 1.2 机器学习组成 1.3 机器学习和其它领域", " 1 学习问题 1.1 什么是机器学习 这里的定义和 Tom Mitchell, 1997 的定义类似。 1.2 机器学习组成 以申请信用卡的问题为例，介绍机器学习问题中的常用记号以及组成成分。 \\(\\mathcal{X}\\)：输入空间 \\(\\mathcal{Y}\\)：输出空间 \\(\\mathcal{D}\\)：训练数据，其中 \\(x_i, y_i (i=1, 2, ..., N)\\) 分别属于空间 \\(\\mathcal{X}, \\mathcal{Y}\\) \\(\\mathcal{H}\\)：假设空间，其中 \\(g \\in \\mathcal{H}\\) 其中，目标函数 \\(f\\) 是一个完全理想化的模式，而这个模式我们是几乎不可能学习到的，所以最终只能得到一个性能比较优的模式 \\(g\\)，使得 \\(g\\) 接近 \\(f\\)。 \\(\\mathcal{A}\\)：学习算法 Learning model = \\(\\mathcal{A}\\) + \\(\\mathcal{H}\\)，一个模型的确定需要两部分来定义，应该这样理解: 比如我们有一个监督式二分类学习问题，\\(\\mathcal{A}\\) 是我们所选择的解决这个问题的算法，比如 Logistic Regression，则 \\(\\mathcal{H}\\) 就是算法 Logistic Regression 对应的所有可能的 \\(g\\) 的空间（即对应的参数和超参数空间），无论好的还是坏的。 1.3 机器学习和其它领域 本节讨论了机器学习分别和数据挖掘、人工智能、统计学的关系，每个学科都有其相近的地方，但又都有其不同的方向。 1.3.1 机器学习和数据挖掘 1.3.2 机器学习和人工智能 1.3.3 机器学习和统计 "],
["-yesno.html", "2 学习回答 Yes/No 2.1 感知假设集（Perceptron Hypothesis Set） 2.2 感知学习算法（Perceptron Learning Algorithm） 2.3 PLA 的确定性（Guarantee of PLA） 2.4 非可分数据集（Non-Separable Data）", " 2 学习回答 Yes/No 2.1 感知假设集（Perceptron Hypothesis Set） 这里还是以是否授予用户信用卡为例，引出一个简化的假设集 \\(\\mathcal{H}\\): Perceptron（感知机）。 感知机假设可以写成向量的形式： 那么感知机 \\(h\\) 会是什么样的呢，可以通过在二维空间构建一个感知机。 感知机在二维空间中是一个线性的二分类器，在多维空间中是分类超平面。 2.2 感知学习算法（Perceptron Learning Algorithm） 空间 \\(\\mathcal{H}\\) 表示所有可能的感知机，我们如何选择一个 \\(g\\) ？ 我们想得到 \\(g \\approx f\\)，但由于我们无法获知 \\(f\\)，所以是很难的 但是我们可以得到在 \\(\\mathcal{D}\\) 上 \\(g \\approx f\\)，更理想的状况下 \\(g(x_n) = f(x_n) = y_n\\) 难点是 \\(\\mathcal{H}\\) 是个无穷集合 方法：从某个 \\(g_0\\) 开始，修正它在 \\(\\mathcal{D}\\) 上犯的错误 下面利用权重向量 \\(\\mathrm{w}_0\\) 来表示某个感知机 \\(g_0\\) \\(\\mathrm{w_t}\\) 表示在第 \\(t\\) 轮，感知机 \\(\\mathrm{w_t}\\) 在点 \\((x_{n(t)}, y_{n(t)})\\) 犯错误，有两类错误： 如果 \\(y_{n(t)}\\) 为正，而预测为负，即 \\(\\mathrm{w_t}^Tx_{n(t)} &lt; 0\\)，则向量 \\(\\mathrm{w_t}\\) 和向量 \\(x_{n(t)}\\) 符号之间夹角为钝角，则通过 \\(\\mathrm{w_{t+1}} = \\mathrm{w_t} + y_nx_{n(t)}\\) 使得和 \\(x_{n(t)}\\) 夹角更小一些； 如果 \\(y_{n(t)}\\) 为负，而预测为正，即 \\(\\mathrm{w_t}^Tx_{n(t)} &gt; 0\\)，则向量 \\(\\mathrm{w_t}\\) 和向量 \\(x_{n(t)}\\) 符号之间夹角为锐角，则通过 \\(\\mathrm{w_{t+1}} = \\mathrm{w_t} + y_nx_{n(t)}\\) 使得和 \\(x_{n(t)}\\) 夹角更大一些； 直到不会犯错误为止，最后得到的 \\(\\mathrm{w}\\) 称之为 \\(\\mathrm{w_{PLA}}\\)。 下面是一道选择题，感觉理解起来还是有点意思的，因此在这里也记录一下。 由于 \\(y_n\\mathrm{w_t}^Tx_n &lt; 0\\)，而 \\(y_n\\mathrm{w_{t+1}}^Tx_n &gt;= y_n\\mathrm{w_t}^Tx_n\\)，说明如果 \\(y_n\\mathrm{w_{t+1}}^Tx_n &gt;= 0\\) 则 \\(\\mathrm{w_{t+1}}\\) 对 \\(x_n\\) 不再犯错，即使仍然犯错，它也比 \\(\\mathrm{w_t}\\) 使得 \\(y_n\\mathrm{w_{t+1}}^Tx_n\\) 更接近 0，仍然是作了一部分修正。 2.3 PLA 的确定性（Guarantee of PLA） PLA 能找到最终不犯错误的 \\(\\mathrm{w}\\)，即 PLA 停止的充分必要条件是：\\(\\mathcal{D}\\) 线性可分。 首先，如果 PLA 停止，也就是存在某个 \\(\\mathrm{w}\\)，使得在 \\(\\mathcal{D}\\) 上不犯错误，则称 \\(\\mathcal{D}\\) 是线性可分的。 那么反过来，假设 \\(\\mathcal{D}\\) 线性可分，那么 PLA 会停止吗？下面来证明： \\(\\mathrm{w_t}\\) Gets More Aligned with \\(\\mathrm{w_f}\\) 可解释为：由于 \\(\\mathrm{w_f}^T\\mathrm{w_{t+1}} &gt; \\mathrm{w_f}^T\\mathrm{w_t}\\)，所以 \\(\\mathrm{w_{t+1}}\\) 比 \\(\\mathrm{w_t}\\) 更接近 \\(\\mathrm{w_f}\\)（可以理解为更近乎相等），即夹角更小；这里有个问题，为什么这种变化不是由于 \\(\\mathrm{w_{t+1}}\\) 比 \\(\\mathrm{w_t}\\) 的模（向量长度）更大引起的呢？下面一幅图给出解释，即通过公式 \\(||\\mathrm{w_{t+1}}||^2 \\le ||\\mathrm{w_t}||^2 + \\max_{n}||y_nx_n||^2\\) 来看，\\(\\mathrm{w_{t+1}}\\) 的长度是有范围的，不会增长的太快。 在上面图片的式子中，可进一步得到 \\[\\begin{align} \\mathrm{w_f}^T\\mathrm{w_{t+1}} &amp; = \\mathrm{w_f}^T(\\mathrm{w_t} + y_{n(t)}x_{n(t)}) \\\\ &amp; \\ge \\mathrm{w_f}^T\\mathrm{w_t} + \\min{n}y_n\\mathrm{w_f}^Tx_n \\\\ &amp; \\ge \\mathrm{w_f}^T\\mathrm{w_0} + (t+1)\\min_{n}y_n\\mathrm{w_f}^Tx_n \\end{align}\\] \\[\\begin{align} ||\\mathrm{w_{t+1}}||^2 &amp; \\le ||\\mathrm{w_t}||^2 + \\max_{n}||y_nx_n||^2 \\\\ &amp; \\le ||\\mathrm{w_0}||^2 + (t+1)\\max_{n}||y_nx_n||^2 \\end{align}\\] 由上面计算的两个式子可以得到： 定义： \\[R^2 = \\max_{n}||x_n||^2\\] \\[\\rho = \\min_{n}y_n\\frac{\\mathrm{w_f}^T}{||\\mathrm{w_f}||}x_n\\] 则从 \\(\\mathrm{w_0} = 0\\) 开始，经过 \\(T\\) 轮错误纠正可以得到： \\[\\begin{align} \\frac{\\mathrm{w_f}^T}{||\\mathrm{w_f}||}\\frac{\\mathrm{w_T}}{||\\mathrm{w_T}||} &amp; \\ge \\frac{T\\min_{n}y_n\\mathrm{w_f}^Tx_n}{||\\mathrm{w_f}||\\sqrt{T}\\max_{n}||y_nx_n||} \\\\ &amp; \\ge \\frac{\\sqrt{T}\\rho}{R} \\\\ &amp; = \\sqrt{T}\\cdot constant \\end{align}\\] 由上面得到的公式，可以直接得到下面练习题的答案： 2.4 非可分数据集（Non-Separable Data） PLA 的优点：实施起来比较简单 缺点是“假设” \\(\\mathcal{D}\\) 是线性可分的，但是，是否线性可分我们并不知道；即使我们知道其线性可分，至于它多久会停下来，我们也不知道。 因此我们需要解决带有噪音的数据上的学习过程，即 \\(\\mathcal{D}\\) 不是线性可分的，因此引入了 Pocket 算法。 以上求解 \\(\\mathrm{w_g}\\) 的问题是 NP-hard 问题。 因此，我们可以退而求其次，使用贪心算法 Pocket 算法来求解： "]
]
